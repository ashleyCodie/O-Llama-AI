# Ollama Backend Express

## Getting started

### Install Ollama
- Copy OllamaSetup.exe from `Z:\SupplementalResources\Install\Ollama` to your downloads. Install it by double-clicking.
- You will need to close any open terminals. Open a terminal and type `ollama help` to see if the isntall worked.
- If that worked run `ollama run mistral`. It should fail, but it will create a .ollama folder in your user directory, which we need.
- When that's done, copy the models folder from `Z:\SupplementalResources\Install\Ollama` into your `C:\Users\PCC-3000\.ollama` folder.
- When that's done close the terminal, re-open it and run `ollama run mistral` again.
- If that works then it should take you to a prompt where you can type in questions using the terminal to the local AI.

### Setup .env
```
EXPRESS_PORT=8081
OLLAMA_API=http://localhost:11434/api/generate
```
- The EXPRESS_PORT is the port Express will run on. Makes sure the frontend uses this same url:port.
- The OLLAMA_API is the default URL that is generated by installing and running Ollama. The chat code will point to the local installation to get AI generated text.

### Install dependencies
```
cd server
npm install
npm run dev
```